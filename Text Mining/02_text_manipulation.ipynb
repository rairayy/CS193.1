{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text manipulation\n",
    "\n",
    "Hello everyone! For this section, we will be learning how to manipulate text data using `TextBlob` and `Scikit-learn`. In particular, we will be using these packages to clean, format, and transform our text data into simpler text and vector representations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob as tb\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read our tweets from the previously created CSV\n",
    "tweets = pd.read_csv('tweets.csv', index_col=None, header=0)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweets):\n",
    "    \"\"\"\n",
    "    Replaces empty tweets, replaces text with lower case characters,\n",
    "    remove special characters and RTs, remove leading and trailing\n",
    "    whitespaces, and remove stopwords.\n",
    "    \"\"\"\n",
    "    tweets['cleaned_text'] = tweets['text'].fillna('')\n",
    "    tweets['cleaned_text'] = tweets['cleaned_text'].str.lower()\n",
    "    tweets['cleaned_text'] = tweets['cleaned_text'].str.replace(r'(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|rt', '')\n",
    "    tweets['cleaned_text'] = tweets['cleaned_text'].str.replace(r'^\\s+|\\s+$', '') \n",
    "    tweets['cleaned_text'] = tweets['cleaned_text'].apply(lambda x: ' '.join([w for w in x.split() if w not in (stopwords)]))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean tweets\n",
    "cleaned_tweets = clean_tweets(tweets)\n",
    "cleaned_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the cleaned tweets into CSV\n",
    "cleaned_tweets.to_csv('cleaned_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_to_dtm(tweets):\n",
    "    tweets = tweets['cleaned_text']\n",
    "    vectorizer = CountVectorizer()\n",
    "    dtm = vectorizer.fit_transform(tweets)\n",
    "    return dtm, vectorizer\n",
    "\n",
    "def tweets_to_ngram(tweets, n=2):\n",
    "    tweets = tweets['cleaned_text']\n",
    "    vectorizer = CountVectorizer(\n",
    "        ngram_range=(n, n),\n",
    "        token_pattern=r'\\b\\w+\\b',\n",
    "        min_df=1)\n",
    "    dtm = vectorizer.fit_transform(tweets)\n",
    "    return dtm, vectorizer\n",
    "\n",
    "def tweets_to_tfidf(tweets):\n",
    "    tweets = tweets['cleaned_text']\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf = vectorizer.fit_transform(tweets)\n",
    "    return tfidf, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get document-term matrix\n",
    "dtm, dtm_v = tweets_to_dtm(cleaned_tweets)\n",
    "dtm.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dtm_v.vocabulary_.items())[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bigram matrix\n",
    "bigram, ngram_v = tweets_to_ngram(cleaned_tweets, n=2)\n",
    "bigram.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ngram_v.vocabulary_.items())[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get TFIDF matrix\n",
    "tfidf, tfidf_v = tweets_to_tfidf(cleaned_tweets)\n",
    "tfidf.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tfidf_v.vocabulary_.items())[0:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
