{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text modeling\n",
    "\n",
    "HI everyone! Today, we're exploring natural language processing. First, we will learn about linguistic features that our documents contain. We will then move on to deeper representations of our corpora and move on to text classification. We will be using `Scikit-learn`, `TextBlob`, and `SpaCy` for this notebook. We'll be taking a look at spam messages for this session. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install `spacy` and its models properly on the **terminal**:\n",
    "\n",
    "`pip3 install spacy`\n",
    "\n",
    "`python3 -m spacy download en_core_web_sm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import messages into dataframe\n",
    "messages = pd.read_csv('SMSSpamCollection', sep='\\t', names=['label', 'message'])\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desribe our dataset\n",
    "messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages['length'] = messages['message'].apply(len)\n",
    "messages.hist(column='length', by='label', bins=50, figsize=(12,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note here that spam messages usually contain more characters compared to real messages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Word, TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the message at index 219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = messages['message'][219]\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(message)\n",
    "blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to split our text data into more meaningful pieces. First, into separate sentences. Second, into separate terms. Generally, we call this process **tokenization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = blob.sentences[1]\n",
    "sentence.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob has a feature that extracts noun phrases, a group of words that form the subject, object, or prepositional object of a sentence. This feature isn't perfect, though. \n",
    "\n",
    "We can also get word counts like last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linguistic features\n",
    "Text naturally has several features of interest. We will take a look at parts of speech and lemmas of terms in sentences. We will also take a look at word synonyms and definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parts of speech** (POS) refer to the function of terms in a given statement (e.g. noun, verb, etc.). POS tagging is a statistical rule-based approach that determines the likely categorization of words in a statement given context. A comprehensive list of tags is found at http://www.clips.ua.ac.be/pages/mbsp-tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence.tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma** are the most rudimentary forms or inflections of words, such as the root of nouns (matrices -> matrix) or present tenses of verbs (threw -> throw). These can be derived through lemmatization. Here in TextBlob, it uses existing lexicons and their respective lemmas to statistically infer the lemmas of provided words. \n",
    "\n",
    "The `lemmatize()` method has an optional parameter:\n",
    "    - 'n' for noun (default)\n",
    "    - 'v' for verb\n",
    "    - 'a' for adjective\n",
    "    - 'r' for adverb (doesn't always work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Word('went')\n",
    "w.lemmatize('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Word('alumnae')\n",
    "w.lemmatize('n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Word('stronger')\n",
    "w.lemmatize('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside TextBlob, we can also pluralize and find synonyms and definitions of words. It can also correct possible spelling errors in text. It can even compare similarities between words! Lastly, it can do translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Word('corpora')\n",
    "w.pluralize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Word('corpora')\n",
    "w.singularize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Word('eat')\n",
    "list(zip(w.synsets, w.definitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Word('havv')\n",
    "w.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = Word('apple').synsets[0]\n",
    "orange = Word('orange').synsets[0]\n",
    "apple.path_similarity(orange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = TextBlob('Je suis Will.')\n",
    "b.detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.translate(from_lang='fr', to='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Get the message at index 518 and make it into a blob. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print the first 5 unique POS tags in blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find all the lemmas (of all nouns and verbs) in blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List all words in blob that are plural (with index of each word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your anwer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except:\n",
    "    print(\"Error loading 'en_core_web_sm' model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](spacy_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(str(sentence))\n",
    "tokens = pd.DataFrame(columns=\n",
    "                      ['text', 'lemma', 'pos', 'tag',\n",
    "                       'dependency', 'shape', 'is_alphabet',\n",
    "                       'is_stopword', 'head_text', 'head_pos'])\n",
    "\n",
    "for token in doc:\n",
    "    data = [token.text, token.lemma_, token.pos_,\n",
    "            token.tag_, token.dep_, token.shape_,\n",
    "            token.is_alpha, token.is_stop,\n",
    "            token.head.text, token.head.pos_]\n",
    "    tokens.loc[len(tokens)] = data\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Apple’s iOS 13 is here — or rather, the public beta for iOS 13 has arrived,giving the masses their first chance to take Apple’s latest operating system for a spin.'\n",
    "doc = nlp(s)\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
